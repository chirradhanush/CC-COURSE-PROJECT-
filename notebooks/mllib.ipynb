{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ICxMfhFXrzX",
        "outputId": "a6f8acd7-e752-4e98-8a1b-af383608b1ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n"
          ]
        }
      ],
      "source": [
        "# Install Java 11 (Spark plays nicely with this in Colab)\n",
        "!apt-get install -y openjdk-11-jdk-headless > /dev/null\n",
        "\n",
        "import os\n",
        "\n",
        "# Set JAVA_HOME *before* importing pyspark\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "\n",
        "# Just to verify Java is there\n",
        "!java -version\n",
        "\n",
        "# Install a recent PySpark compatible with Java 11\n",
        "!pip install -q pyspark==3.5.1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "import math\n",
        "\n",
        "spark = SparkSession.builder.appName(\"smart_parking_ml\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "spark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "IFNCmDo4Xx7Q",
        "outputId": "f12be2b8-6e7e-4d61-d4fb-5f8ea7b38760"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e5c0e4d79e0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://3f967ba28143:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>smart_parking_ml</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/parking_ml_ready.csv\"  # adjust if different\n",
        "\n",
        "df_raw = (\n",
        "    spark.read\n",
        "    .option(\"header\", True)\n",
        "    .option(\"inferSchema\", True)\n",
        "    .csv(input_path)\n",
        ")\n",
        "\n",
        "df_raw.printSchema()\n",
        "df_raw.show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b75X0TqoYvny",
        "outputId": "b6eae73c-d1df-4033-9e75-8489bfddeb01"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- device_id: integer (nullable = true)\n",
            " |-- street_marker: string (nullable = true)\n",
            " |-- sign_plate_id: string (nullable = true)\n",
            " |-- target_occupied: integer (nullable = true)\n",
            " |-- hour: double (nullable = true)\n",
            " |-- dow: double (nullable = true)\n",
            " |-- is_weekend: integer (nullable = true)\n",
            " |-- duration_min: double (nullable = true)\n",
            "\n",
            "+---------+-------------+-------------+---------------+----+---+----------+-------------------+\n",
            "|device_id|street_marker|sign_plate_id|target_occupied|hour|dow|is_weekend|duration_min       |\n",
            "+---------+-------------+-------------+---------------+----+---+----------+-------------------+\n",
            "|23913    |1581S        |nan          |1              |4.0 |6.0|1         |0.6                |\n",
            "|23913    |1581S        |3.0          |1              |16.0|6.0|1         |0.43333333333333335|\n",
            "|23913    |1581S        |3.0          |1              |18.0|6.0|1         |0.23333333333333334|\n",
            "|23913    |1581S        |99.0         |0              |9.0 |1.0|0         |79.2               |\n",
            "|23913    |1581S        |99.0         |1              |10.0|1.0|0         |13.833333333333334 |\n",
            "+---------+-------------+-------------+---------------+----+---+----------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename target_occupied -> label and basic cleaning\n",
        "df = (\n",
        "    df_raw\n",
        "    .withColumnRenamed(\"target_occupied\", \"label\")\n",
        ")\n",
        "\n",
        "# Make sure label is numeric (0/1) and drop rows with missing key fields\n",
        "df = (\n",
        "    df\n",
        "    .withColumn(\"label\", F.col(\"label\").cast(\"double\"))\n",
        "    .dropna(subset=[\"label\", \"street_marker\", \"sign_plate_id\", \"hour\", \"dow\", \"duration_min\"])\n",
        ")\n",
        "\n",
        "# Basic features\n",
        "df = df.withColumn(\n",
        "    \"log_duration_min\",\n",
        "    F.log1p(F.col(\"duration_min\"))\n",
        ")\n",
        "\n",
        "# Discrete hour bucket (for interpretability, if you want to inspect later)\n",
        "df = df.withColumn(\n",
        "    \"hour_bucket\",\n",
        "    F.when((F.col(\"hour\") >= 7) & (F.col(\"hour\") <= 10), F.lit(1))\n",
        "     .when((F.col(\"hour\") >= 16) & (F.col(\"hour\") <= 19), F.lit(2))\n",
        "     .otherwise(F.lit(0))\n",
        ")\n",
        "\n",
        "# Peak-hour flag\n",
        "df = df.withColumn(\n",
        "    \"is_peak\",\n",
        "    F.when(F.col(\"hour_bucket\") != 0, F.lit(1)).otherwise(F.lit(0))\n",
        ")\n",
        "\n",
        "print(\"Schema after basic cleaning & features:\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"Label distribution:\")\n",
        "df.groupBy(\"label\").count().orderBy(\"label\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgSgKbMdY4M0",
        "outputId": "fb7396b2-47cc-426c-83f3-b4ff34a6d248"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema after basic cleaning & features:\n",
            "root\n",
            " |-- device_id: integer (nullable = true)\n",
            " |-- street_marker: string (nullable = true)\n",
            " |-- sign_plate_id: string (nullable = true)\n",
            " |-- label: double (nullable = true)\n",
            " |-- hour: double (nullable = true)\n",
            " |-- dow: double (nullable = true)\n",
            " |-- is_weekend: integer (nullable = true)\n",
            " |-- duration_min: double (nullable = true)\n",
            " |-- log_duration_min: double (nullable = true)\n",
            " |-- hour_bucket: integer (nullable = false)\n",
            " |-- is_peak: integer (nullable = false)\n",
            "\n",
            "Label distribution:\n",
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|  0.0| 7500|\n",
            "|  1.0| 7499|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start from cleaned df\n",
        "df_fe = df\n",
        "\n",
        "# 1) Cyclical encoding for HOUR (0â€“23)\n",
        "df_fe = df_fe.withColumn(\n",
        "    \"hour_sin\",\n",
        "    F.sin(F.lit(2 * math.pi) * F.col(\"hour\") / F.lit(24.0))\n",
        ").withColumn(\n",
        "    \"hour_cos\",\n",
        "    F.cos(F.lit(2 * math.pi) * F.col(\"hour\") / F.lit(24.0))\n",
        ")\n",
        "\n",
        "# 2) Cyclical encoding for DOW (0â€“6 or 1â€“7)\n",
        "df_fe = df_fe.withColumn(\n",
        "    \"dow_sin\",\n",
        "    F.sin(F.lit(2 * math.pi) * F.col(\"dow\") / F.lit(7.0))\n",
        ").withColumn(\n",
        "    \"dow_cos\",\n",
        "    F.cos(F.lit(2 * math.pi) * F.col(\"dow\") / F.lit(7.0))\n",
        ")\n",
        "\n",
        "# 3) Aggregate stats per street_marker\n",
        "marker_stats = (\n",
        "    df_fe.groupBy(\"street_marker\")\n",
        "    .agg(\n",
        "        F.avg(\"duration_min\").alias(\"avg_duration_per_marker\"),\n",
        "        F.avg(\"label\").alias(\"occ_rate_per_marker\")\n",
        "    )\n",
        ")\n",
        "\n",
        "df_fe = df_fe.join(marker_stats, on=\"street_marker\", how=\"left\")\n",
        "\n",
        "print(\"Schema AFTER feature engineering:\")\n",
        "df_fe.printSchema()\n",
        "\n",
        "df_fe.select(\n",
        "    \"street_marker\", \"hour\", \"dow\",\n",
        "    \"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\",\n",
        "    \"avg_duration_per_marker\", \"occ_rate_per_marker\", \"label\"\n",
        ").show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyLBSSdDZHWc",
        "outputId": "2006b8b0-3f03-482e-e6a1-5eb9fb2f026f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema AFTER feature engineering:\n",
            "root\n",
            " |-- street_marker: string (nullable = true)\n",
            " |-- device_id: integer (nullable = true)\n",
            " |-- sign_plate_id: string (nullable = true)\n",
            " |-- label: double (nullable = true)\n",
            " |-- hour: double (nullable = true)\n",
            " |-- dow: double (nullable = true)\n",
            " |-- is_weekend: integer (nullable = true)\n",
            " |-- duration_min: double (nullable = true)\n",
            " |-- log_duration_min: double (nullable = true)\n",
            " |-- hour_bucket: integer (nullable = false)\n",
            " |-- is_peak: integer (nullable = false)\n",
            " |-- hour_sin: double (nullable = true)\n",
            " |-- hour_cos: double (nullable = true)\n",
            " |-- dow_sin: double (nullable = true)\n",
            " |-- dow_cos: double (nullable = true)\n",
            " |-- avg_duration_per_marker: double (nullable = true)\n",
            " |-- occ_rate_per_marker: double (nullable = true)\n",
            "\n",
            "+-------------+----+---+-------------------+-----------------------+-------------------+------------------+-----------------------+-------------------+-----+\n",
            "|street_marker|hour|dow|hour_sin           |hour_cos               |dow_sin            |dow_cos           |avg_duration_per_marker|occ_rate_per_marker|label|\n",
            "+-------------+----+---+-------------------+-----------------------+-------------------+------------------+-----------------------+-------------------+-----+\n",
            "|1581S        |4.0 |6.0|0.8660254037844386 |0.5000000000000001     |-0.7818314824680299|0.6234898018587334|18.659395424836593     |0.47058823529411764|1.0  |\n",
            "|1581S        |16.0|6.0|-0.8660254037844384|-0.5000000000000004    |-0.7818314824680299|0.6234898018587334|18.659395424836593     |0.47058823529411764|1.0  |\n",
            "|1581S        |18.0|6.0|-1.0               |-1.8369701987210297E-16|-0.7818314824680299|0.6234898018587334|18.659395424836593     |0.47058823529411764|1.0  |\n",
            "|1581S        |9.0 |1.0|0.7071067811865476 |-0.7071067811865475    |0.7818314824680298 |0.6234898018587336|18.659395424836593     |0.47058823529411764|0.0  |\n",
            "|1581S        |10.0|1.0|0.49999999999999994|-0.8660254037844387    |0.7818314824680298 |0.6234898018587336|18.659395424836593     |0.47058823529411764|1.0  |\n",
            "+-------------+----+---+-------------------+-----------------------+-------------------+------------------+-----------------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_model = df_fe\n"
      ],
      "metadata": {
        "id": "965Nvzz_ZMQ_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Categorical columns (treated as categories, not numbers)\n",
        "cat_cols = [\"street_marker\", \"sign_plate_id\",\"device_id\"]\n",
        "\n",
        "indexers = [\n",
        "    StringIndexer(\n",
        "        inputCol=c,\n",
        "        outputCol=f\"{c}_idx\",\n",
        "        handleInvalid=\"keep\"   # avoid errors on unseen categories\n",
        "    )\n",
        "    for c in cat_cols\n",
        "]\n",
        "\n",
        "# Numeric + engineered features\n",
        "numeric_features = [\n",
        "    \"hour\",\n",
        "    \"dow\",\n",
        "    \"is_weekend\",\n",
        "    \"duration_min\",\n",
        "    \"log_duration_min\",\n",
        "    \"is_peak\",\n",
        "    \"hour_sin\",\n",
        "    \"hour_cos\",\n",
        "    \"dow_sin\",\n",
        "    \"dow_cos\",\n",
        "    \"avg_duration_per_marker\",\n",
        "    \"occ_rate_per_marker\",\n",
        "]\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=numeric_features + [f\"{c}_idx\" for c in cat_cols],\n",
        "    outputCol=\"features_raw\"\n",
        ")\n",
        "\n",
        "scaler = StandardScaler(\n",
        "    inputCol=\"features_raw\",\n",
        "    outputCol=\"features\",\n",
        "    withMean=False,\n",
        "    withStd=True\n",
        ")\n",
        "\n",
        "preprocess_pipeline = Pipeline(stages=indexers + [assembler, scaler])\n",
        "\n",
        "preprocess_model = preprocess_pipeline.fit(df_model)\n",
        "df_prepared = preprocess_model.transform(df_model)\n",
        "\n",
        "df_prepared.select(\"features\", \"label\").show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZslM1Rb7ZQza",
        "outputId": "71c2162f-8675-41ff-e7b7-6d75afec039d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "|features                                                                                                                                                                                                                                                                                             |label|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "|[0.6727401171714494,3.21262444817411,2.4250446719252436,0.005771956227670986,0.27406169860009777,0.0,1.2519684107969538,0.7512718770317408,-1.1322580925994379,0.8709802312242307,0.40066298148290974,5.975094793448066,3.0120090547578027,0.0,3.0120090547578027]                                   |1.0  |\n",
            "|[2.6909604686857977,3.21262444817411,2.4250446719252436,0.004168635053317935,0.20991957221194546,2.005186588125704,-1.2519684107969533,-0.7512718770317413,-1.1322580925994379,0.8709802312242307,0.40066298148290974,5.975094793448066,3.0120090547578027,2.2218475239020985,3.0120090547578027]    |1.0  |\n",
            "|[3.0273305272715225,3.21262444817411,2.4250446719252436,0.0022446496440942727,0.12228919390366978,2.005186588125704,-1.4456485979810585,-2.760128098489035E-16,-1.1322580925994379,0.8709802312242307,0.40066298148290974,5.975094793448066,3.0120090547578027,2.2218475239020985,3.0120090547578027]|1.0  |\n",
            "|[1.5136652636357613,0.5354374080290184,0.0,0.7618982220525703,2.556639751838517,2.005186588125704,1.0222279268452317,-1.0624588775277795,1.1322580925994377,0.8709802312242311,0.40066298148290974,5.975094793448066,3.0120090547578027,3.9499511536037306,3.0120090547578027]                       |0.0  |\n",
            "|[1.6818502929286236,0.5354374080290184,0.0,0.13307565747130332,1.572563738398457,2.005186588125704,0.7228242989905291,-1.3012410613166125,1.1322580925994377,0.8709802312242311,0.40066298148290974,5.975094793448066,3.0120090547578027,3.9499511536037306,3.0120090547578027]                      |1.0  |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = df_prepared.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "print(\"Train rows:\", train_df.count())\n",
        "print(\"Test rows:\", test_df.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64dOUdymZUIZ",
        "outputId": "d00d658e-906b-4bc4-ac4c-15ba8e0c7693"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train rows: 10616\n",
            "Test rows: 4383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
        "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# ðŸ”¹ (Optional but helpful) Cache to speed up and reduce recomputation\n",
        "train_df = train_df.cache()\n",
        "test_df = test_df.cache()\n",
        "\n",
        "# ============================================================\n",
        "# 1ï¸âƒ£ Tuned Random Forest (your original code)\n",
        "# ============================================================\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    seed=42,\n",
        "    featureSubsetStrategy=\"sqrt\",  # common best practice\n",
        "    probabilityCol=\"rf_prob\"       # <-- important for stacking\n",
        ")\n",
        "\n",
        "# We will optimize for AUC\n",
        "evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    rawPredictionCol=\"rawPrediction\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "# ðŸ”¹ Lightweight param grid (ONLY 4 combinations)\n",
        "paramGrid = (\n",
        "    ParamGridBuilder()\n",
        "      .addGrid(rf.numTrees, [100, 200])   # 2 options\n",
        "      .addGrid(rf.maxDepth, [8, 12])      # 2 options\n",
        "      .build()                            # 2 * 2 = 4 total\n",
        ")\n",
        "\n",
        "# ðŸ”¹ TrainValidationSplit instead of CrossValidator\n",
        "tvs = TrainValidationSplit(\n",
        "    estimator=rf,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    trainRatio=0.8,   # 80% of train_df used for training each model, 20% for validation\n",
        "    parallelism=1     # keep this 1 to reduce memory pressure\n",
        ")\n",
        "\n",
        "# ðŸš€ Fit the Tuning\n",
        "tvs_model = tvs.fit(train_df)\n",
        "best_rf_model = tvs_model.bestModel\n",
        "\n",
        "print(\"Best Random Forest params:\")\n",
        "print(\"  numTrees:\", best_rf_model.getOrDefault(\"numTrees\"))\n",
        "print(\"  maxDepth:\", best_rf_model.getOrDefault(\"maxDepth\"))\n",
        "\n",
        "# ðŸ”¹ Evaluate RF on the held-out test_df\n",
        "rf_preds = best_rf_model.transform(test_df)\n",
        "\n",
        "auc_eval = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    rawPredictionCol=\"rawPrediction\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "acc_eval = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "f1_eval = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"f1\"\n",
        ")\n",
        "\n",
        "auc = auc_eval.evaluate(rf_preds)\n",
        "acc = acc_eval.evaluate(rf_preds)\n",
        "f1 = f1_eval.evaluate(rf_preds)\n",
        "\n",
        "print(\"\\n==== Tuned Random Forest (TrainValidationSplit) ====\")\n",
        "print(f\"AUC:      {auc:.4f}\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"F1:       {f1:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion matrix (prediction vs label):\")\n",
        "rf_preds.groupBy(\"prediction\", \"label\").count().orderBy(\"prediction\", \"label\").show()\n",
        "\n",
        "# ============================================================\n",
        "# 2ï¸âƒ£ Second base model: Logistic Regression\n",
        "# ============================================================\n",
        "\n",
        "lr_base = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    maxIter=50,\n",
        "    regParam=0.01,\n",
        "    probabilityCol=\"lr_prob\",\n",
        "    rawPredictionCol=\"lr_raw\",\n",
        "    predictionCol=\"lr_pred\"\n",
        ")\n",
        "\n",
        "lr_base_model = lr_base.fit(train_df)\n",
        "\n",
        "# ============================================================\n",
        "# 3ï¸âƒ£ Build level-1 (stacking) training data\n",
        "#     Features for meta-learner = [RF_prob1, LR_prob1]\n",
        "# ============================================================\n",
        "\n",
        "# Apply both base models on train_df\n",
        "train_lvl1 = best_rf_model.transform(train_df)      # adds rf_prob\n",
        "train_lvl1 = lr_base_model.transform(train_lvl1)    # adds lr_prob\n",
        "\n",
        "# UDF to extract P(class=1) from probability vector\n",
        "vec_to_prob1_udf = F.udf(lambda v: float(v[1]), DoubleType())\n",
        "\n",
        "train_lvl1 = (\n",
        "    train_lvl1\n",
        "        .withColumn(\"rf_prob1\", vec_to_prob1_udf(\"rf_prob\"))\n",
        "        .withColumn(\"lr_prob1\", vec_to_prob1_udf(\"lr_prob\"))\n",
        ")\n",
        "\n",
        "# Assemble stacking features\n",
        "stack_assembler = VectorAssembler(\n",
        "    inputCols=[\"rf_prob1\", \"lr_prob1\"],\n",
        "    outputCol=\"stack_features\"\n",
        ")\n",
        "\n",
        "train_lvl1 = stack_assembler.transform(train_lvl1)\n",
        "\n",
        "# Meta-learner on top of RF + LR probabilities\n",
        "stack_lr = LogisticRegression(\n",
        "    featuresCol=\"stack_features\",\n",
        "    labelCol=\"label\",\n",
        "    maxIter=50,\n",
        "    regParam=0.0,\n",
        "    probabilityCol=\"stack_prob\",\n",
        "    rawPredictionCol=\"stack_raw\",\n",
        "    predictionCol=\"stack_pred\"\n",
        ")\n",
        "\n",
        "stack_model = stack_lr.fit(train_lvl1)\n",
        "\n",
        "# ============================================================\n",
        "# 4ï¸âƒ£ Apply stacking model on test set\n",
        "# ============================================================\n",
        "\n",
        "test_lvl1 = best_rf_model.transform(test_df)\n",
        "test_lvl1 = lr_base_model.transform(test_lvl1)\n",
        "\n",
        "test_lvl1 = (\n",
        "    test_lvl1\n",
        "        .withColumn(\"rf_prob1\", vec_to_prob1_udf(\"rf_prob\"))\n",
        "        .withColumn(\"lr_prob1\", vec_to_prob1_udf(\"lr_prob\"))\n",
        ")\n",
        "\n",
        "test_lvl1 = stack_assembler.transform(test_lvl1)\n",
        "\n",
        "stack_preds = stack_model.transform(test_lvl1)\n",
        "\n",
        "# ============================================================\n",
        "# 5ï¸âƒ£ Evaluate stacking model\n",
        "# ============================================================\n",
        "\n",
        "stack_auc_eval = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    rawPredictionCol=\"stack_raw\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "stack_acc_eval = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"stack_pred\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "stack_f1_eval = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"stack_pred\",\n",
        "    metricName=\"f1\"\n",
        ")\n",
        "\n",
        "stack_auc = stack_auc_eval.evaluate(stack_preds)\n",
        "stack_acc = stack_acc_eval.evaluate(stack_preds)\n",
        "stack_f1 = stack_f1_eval.evaluate(stack_preds)\n",
        "\n",
        "print(\"\\n==== Stacking Model (meta LR on [RF_prob1, LR_prob1]) ====\")\n",
        "print(f\"AUC:      {stack_auc:.4f}\")\n",
        "print(f\"Accuracy: {stack_acc:.4f}\")\n",
        "print(f\"F1:       {stack_f1:.4f}\")\n",
        "\n",
        "print(\"\\nStacking confusion matrix (stack_pred vs label):\")\n",
        "stack_preds.groupBy(\"stack_pred\", \"label\").count().orderBy(\"stack_pred\", \"label\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWXqmqlMIRQK",
        "outputId": "d272ac91-cd55-4f25-fa47-f45ff109f521"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Random Forest params:\n",
            "  numTrees: 100\n",
            "  maxDepth: 12\n",
            "\n",
            "==== Tuned Random Forest (TrainValidationSplit) ====\n",
            "AUC:      0.7968\n",
            "Accuracy: 0.7157\n",
            "F1:       0.7158\n",
            "\n",
            "Confusion matrix (prediction vs label):\n",
            "+----------+-----+-----+\n",
            "|prediction|label|count|\n",
            "+----------+-----+-----+\n",
            "|       0.0|  0.0| 1555|\n",
            "|       0.0|  1.0|  646|\n",
            "|       1.0|  0.0|  603|\n",
            "|       1.0|  1.0| 1590|\n",
            "+----------+-----+-----+\n",
            "\n",
            "\n",
            "==== Stacking Model (meta LR on [RF_prob1, LR_prob1]) ====\n",
            "AUC:      0.7795\n",
            "Accuracy: 0.7000\n",
            "F1:       0.7001\n",
            "\n",
            "Stacking confusion matrix (stack_pred vs label):\n",
            "+----------+-----+-----+\n",
            "|stack_pred|label|count|\n",
            "+----------+-----+-----+\n",
            "|       0.0|  0.0| 1535|\n",
            "|       0.0|  1.0|  695|\n",
            "|       1.0|  0.0|  623|\n",
            "|       1.0|  1.0| 1541|\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RF Tuned\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# ðŸ”¹ (Optional but helpful) Cache to speed up and reduce recomputation\n",
        "train_df = train_df.cache()\n",
        "test_df = test_df.cache()\n",
        "\n",
        "# Base Random Forest (we will tune only a couple of params)\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    seed=42,\n",
        "    featureSubsetStrategy=\"sqrt\"  # common best practice\n",
        ")\n",
        "\n",
        "# We will optimize for AUC\n",
        "evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    rawPredictionCol=\"rawPrediction\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "# ðŸ”¹ Lightweight param grid (ONLY 4 combinations)\n",
        "paramGrid = (\n",
        "    ParamGridBuilder()\n",
        "      .addGrid(rf.numTrees, [100, 200])   # 2 options\n",
        "      .addGrid(rf.maxDepth, [8, 12])      # 2 options\n",
        "      .build()                            # 2 * 2 = 4 total\n",
        ")\n",
        "\n",
        "# ðŸ”¹ TrainValidationSplit instead of CrossValidator\n",
        "tvs = TrainValidationSplit(\n",
        "    estimator=rf,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    trainRatio=0.8,   # 80% of train_df used for training each model, 20% for validation\n",
        "    parallelism=1     # keep this 1 to reduce memory pressure\n",
        ")\n",
        "\n",
        "# ðŸš€ Fit the Tuning\n",
        "tvs_model = tvs.fit(train_df)\n",
        "best_rf_model = tvs_model.bestModel\n",
        "\n",
        "print(\"Best Random Forest params:\")\n",
        "print(\"  numTrees:\", best_rf_model.getOrDefault(\"numTrees\"))\n",
        "print(\"  maxDepth:\", best_rf_model.getOrDefault(\"maxDepth\"))\n",
        "\n",
        "# ðŸ”¹ Evaluate on the held-out test_df (never seen in training/validation)\n",
        "rf_preds = best_rf_model.transform(test_df)\n",
        "\n",
        "auc_eval = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    rawPredictionCol=\"rawPrediction\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "acc_eval = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "f1_eval = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"f1\"\n",
        ")\n",
        "\n",
        "auc = auc_eval.evaluate(rf_preds)\n",
        "acc = acc_eval.evaluate(rf_preds)\n",
        "f1 = f1_eval.evaluate(rf_preds)\n",
        "\n",
        "print(\"\\n==== Tuned Random Forest (TrainValidationSplit) ====\")\n",
        "print(f\"AUC:      {auc:.4f}\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"F1:       {f1:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion matrix (prediction vs label):\")\n",
        "rf_preds.groupBy(\"prediction\", \"label\").count().orderBy(\"prediction\", \"label\").show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pPRxkg4quws",
        "outputId": "2944fbd5-20cf-46e9-bcf1-96dae432c427"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Random Forest params:\n",
            "  numTrees: 200\n",
            "  maxDepth: 12\n",
            "\n",
            "==== Tuned Random Forest (TrainValidationSplit) ====\n",
            "AUC:      0.7350\n",
            "Accuracy: 0.6596\n",
            "F1:       0.6596\n",
            "\n",
            "Confusion matrix (prediction vs label):\n",
            "+----------+-----+-----+\n",
            "|prediction|label|count|\n",
            "+----------+-----+-----+\n",
            "|       0.0|  0.0| 1425|\n",
            "|       0.0|  1.0|  733|\n",
            "|       1.0|  0.0|  759|\n",
            "|       1.0|  1.0| 1466|\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "\n",
        "def train_and_eval(estimator, name: str):\n",
        "    print(f\"\\n==== {name} ====\")\n",
        "    model = estimator.fit(train_df)\n",
        "    preds = model.transform(test_df)\n",
        "\n",
        "    # AUC\n",
        "    auc_eval = BinaryClassificationEvaluator(\n",
        "        labelCol=\"label\",\n",
        "        rawPredictionCol=\"rawPrediction\",\n",
        "        metricName=\"areaUnderROC\"\n",
        "    )\n",
        "    auc = auc_eval.evaluate(preds)\n",
        "\n",
        "    # Accuracy & F1 (multi-class evaluator works fine for binary)\n",
        "    acc_eval = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"accuracy\"\n",
        "    )\n",
        "    f1_eval = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"f1\"\n",
        "    )\n",
        "\n",
        "    acc = acc_eval.evaluate(preds)\n",
        "    f1 = f1_eval.evaluate(preds)\n",
        "\n",
        "    print(f\"AUC:      {auc:.4f}\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"F1:       {f1:.4f}\")\n",
        "\n",
        "    print(\"Confusion matrix (prediction vs label):\")\n",
        "    preds.groupBy(\"prediction\", \"label\").count().orderBy(\"prediction\", \"label\").show()\n",
        "\n",
        "    return model, preds\n"
      ],
      "metadata": {
        "id": "Zhdh3Vh6ZYaR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    maxIter=50,\n",
        "    regParam=0.01,\n",
        "    elasticNetParam=0.0\n",
        ")\n",
        "\n",
        "lr_model, lr_preds = train_and_eval(lr, \"Logistic Regression\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwBd-LCoZcu1",
        "outputId": "40adc92f-7323-4bad-a842-0fc14c3eeb51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Logistic Regression ====\n",
            "AUC:      0.5829\n",
            "Accuracy: 0.5494\n",
            "F1:       0.5491\n",
            "Confusion matrix (prediction vs label):\n",
            "+----------+-----+-----+\n",
            "|prediction|label|count|\n",
            "+----------+-----+-----+\n",
            "|       0.0|  0.0| 1147|\n",
            "|       0.0|  1.0|  938|\n",
            "|       1.0|  0.0| 1037|\n",
            "|       1.0|  1.0| 1261|\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    numTrees=100,\n",
        "    maxDepth=10,\n",
        "    minInstancesPerNode=5,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "rf_model, rf_preds = train_and_eval(rf, \"Random Forest\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BCbhmDEZf7l",
        "outputId": "c77c0513-8f2f-45b5-f979-58bd6f8c5d33"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Random Forest ====\n",
            "AUC:      0.7297\n",
            "Accuracy: 0.6623\n",
            "F1:       0.6623\n",
            "Confusion matrix (prediction vs label):\n",
            "+----------+-----+-----+\n",
            "|prediction|label|count|\n",
            "+----------+-----+-----+\n",
            "|       0.0|  0.0| 1416|\n",
            "|       0.0|  1.0|  712|\n",
            "|       1.0|  0.0|  768|\n",
            "|       1.0|  1.0| 1487|\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix for 'JavaPackage' object is not callable error after installing SynapseML\n",
        "\n",
        "# Stop existing SparkSession if running\n",
        "if 'spark' in locals() and spark is not None:\n",
        "    spark.stop()\n",
        "\n",
        "# Re-initialize SparkSession to ensure SynapseML jars are picked up\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"smart_parking_ml\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"SparkSession restarted.\")\n",
        "\n",
        "# Re-run the data preparation steps to get df_prepared, train_df, and test_df\n",
        "# as existing DataFrames will be linked to the old SparkSession.\n",
        "\n",
        "# Reload the raw data\n",
        "input_path = \"/content/smart_parking_clean.csv\"\n",
        "df_full = (\n",
        "    spark.read\n",
        "        .option(\"header\", True)\n",
        "        .option(\"inferSchema\", True)\n",
        "        .csv(input_path)\n",
        ")\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "df_full = df_full.withColumnRenamed(\"target_occupied\", \"label\")\n",
        "\n",
        "# Drop leakage columns\n",
        "leaky_cols = [\"Status\", \"_occupied\", \"_occupied_imp\"]\n",
        "for c in leaky_cols:\n",
        "    if c in df_full.columns:\n",
        "        df_full = df_full.drop(c)\n",
        "\n",
        "numeric_features = [\n",
        "    \"Hour\", \"DayOfWeek\", \"IsWeekend\", \"Month\", \"DayOfMonth\",\n",
        "    \"DwellMinutes\", \"InViolation\",\n",
        "    \"_duration_min\", \"_hour\", \"_dow\", \"_is_weekend\",\n",
        "    \"_duration_min_w\", \"_duration_robust_z\",\n",
        "    \"rolling_occ_N10\", \"arrivals_N10\"\n",
        "]\n",
        "\n",
        "cat_features = [\n",
        "    \"PartOfDay\", \"AreaName\", \"SideName\", \"SideOfStreetCode\",\n",
        "    \"street_marker_lumped\", \"device_id_lumped\"\n",
        "]\n",
        "\n",
        "df_full = df_full.dropna(subset=[\"label\"] + numeric_features + cat_features)\n",
        "df_full = df_full.withColumn(\"label\", F.col(\"label\").cast(\"double\"))\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "indexers = [\n",
        "    StringIndexer(\n",
        "        inputCol=c,\n",
        "        outputCol=f\"{c}_idx\",\n",
        "        handleInvalid=\"keep\"\n",
        "    )\n",
        "    for c in cat_features\n",
        "]\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=numeric_features + [f\"{c}_idx\" for c in cat_features],\n",
        "    outputCol=\"features_raw\"\n",
        ")\n",
        "\n",
        "scaler = StandardScaler(\n",
        "    inputCol=\"features_raw\",\n",
        "    outputCol=\"features\",\n",
        "    withMean=False,\n",
        "    withStd=True\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=indexers + [assembler, scaler])\n",
        "preprocess_model = pipeline.fit(df_full)\n",
        "df_prepared = preprocess_model.transform(df_full)\n",
        "\n",
        "train_df, test_df = df_prepared.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "print(\"DataFrames 'train_df' and 'test_df' re-prepared successfully.\")\n",
        "print(\"Please re-run the subsequent cells, starting with the LightGBM training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU7OV6fpqK-W",
        "outputId": "c6eb4174-f6f1-4f1f-fa4e-94652316c65c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession restarted.\n",
            "DataFrames 'train_df' and 'test_df' re-prepared successfully.\n",
            "Please re-run the subsequent cells, starting with the LightGBM training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "gbt = GBTClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    maxIter=80,\n",
        "    maxDepth=5,\n",
        "    stepSize=0.1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "gbt_model, gbt_preds = train_and_eval(gbt, \"Gradient-Boosted Trees\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soH2XFegZoZr",
        "outputId": "52359495-dd59-4395-fc47-f83c182bd409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Gradient-Boosted Trees ====\n",
            "AUC:      0.7265\n",
            "Accuracy: 0.6616\n",
            "F1:       0.6616\n",
            "Confusion matrix (prediction vs label):\n",
            "+----------+-----+-----+\n",
            "|prediction|label|count|\n",
            "+----------+-----+-----+\n",
            "|       0.0|  0.0| 1430|\n",
            "|       0.0|  1.0|  729|\n",
            "|       1.0|  0.0|  754|\n",
            "|       1.0|  1.0| 1470|\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "\n",
        "svc = LinearSVC(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    maxIter=50,\n",
        "    regParam=0.1\n",
        ")\n",
        "\n",
        "svc_model, svc_preds = train_and_eval(svc, \"Linear SVM (LinearSVC)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs3o8iVlZzGn",
        "outputId": "27e6823a-1159-4358-8412-ee6dabb9ff20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Linear SVM (LinearSVC) ====\n",
            "AUC:      0.5842\n",
            "Accuracy: 0.5530\n",
            "F1:       0.5529\n",
            "Confusion matrix (prediction vs label):\n",
            "+----------+-----+-----+\n",
            "|prediction|label|count|\n",
            "+----------+-----+-----+\n",
            "|       0.0|  0.0| 1173|\n",
            "|       0.0|  1.0|  948|\n",
            "|       1.0|  0.0| 1011|\n",
            "|       1.0|  1.0| 1251|\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/smart_parking_clean.csv\"\n",
        "\n",
        "df_full = (\n",
        "    spark.read\n",
        "        .option(\"header\", True)\n",
        "        .option(\"inferSchema\", True)\n",
        "        .csv(input_path)\n",
        ")\n",
        "\n",
        "df_full.printSchema()\n",
        "df_full.show(5, truncate=False)\n"
      ],
      "metadata": {
        "id": "OIuoNR8KaCwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcbc1ad8-e16b-4e4b-f5f2-3664f11af10d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- BayId: integer (nullable = true)\n",
            " |-- DeviceId: integer (nullable = true)\n",
            " |-- StreetMarker: string (nullable = true)\n",
            " |-- StreetName: string (nullable = true)\n",
            " |-- Sign: string (nullable = true)\n",
            " |-- Status: string (nullable = true)\n",
            " |-- ArrivalTime: timestamp (nullable = true)\n",
            " |-- DepartureTime: timestamp (nullable = true)\n",
            " |-- DurationSeconds: double (nullable = true)\n",
            " |-- Hour: integer (nullable = true)\n",
            " |-- DayOfWeek: integer (nullable = true)\n",
            " |-- IsWeekend: boolean (nullable = true)\n",
            " |-- Month: integer (nullable = true)\n",
            " |-- DayOfMonth: integer (nullable = true)\n",
            " |-- PartOfDay: string (nullable = true)\n",
            " |-- DwellMinutes: double (nullable = true)\n",
            " |-- AreaName: string (nullable = true)\n",
            " |-- StreetId: integer (nullable = true)\n",
            " |-- InViolation: boolean (nullable = true)\n",
            " |-- SignPlateID: string (nullable = true)\n",
            " |-- SideName: string (nullable = true)\n",
            " |-- SideOfStreet: integer (nullable = true)\n",
            " |-- SideOfStreetCode: string (nullable = true)\n",
            " |-- _duration_min: double (nullable = true)\n",
            " |-- _occupied: double (nullable = true)\n",
            " |-- _event_time: timestamp (nullable = true)\n",
            " |-- _hour: double (nullable = true)\n",
            " |-- _dow: double (nullable = true)\n",
            " |-- _is_weekend: integer (nullable = true)\n",
            " |-- _duration_outlier: integer (nullable = true)\n",
            " |-- _duration_min_w: double (nullable = true)\n",
            " |-- _duration_robust_z: double (nullable = true)\n",
            " |-- street_marker_lumped: string (nullable = true)\n",
            " |-- device_id_lumped: string (nullable = true)\n",
            " |-- _duration_min_imp: double (nullable = true)\n",
            " |-- _occupied_imp: integer (nullable = true)\n",
            " |-- target_occupied: integer (nullable = true)\n",
            " |-- rolling_occ_N10: double (nullable = true)\n",
            " |-- arrivals_N10: double (nullable = true)\n",
            "\n",
            "+-----+--------+------------+---------------+---------------------+----------+-------------------+-------------------+---------------+----+---------+---------+-----+----------+---------+------------+--------+--------+-----------+-----------+--------+------------+----------------+-------------------+---------+-------------------+-----+----+-----------+-----------------+-------------------+-------------------+--------------------+----------------+-------------------+-------------+---------------+---------------+------------+\n",
            "|BayId|DeviceId|StreetMarker|StreetName     |Sign                 |Status    |ArrivalTime        |DepartureTime      |DurationSeconds|Hour|DayOfWeek|IsWeekend|Month|DayOfMonth|PartOfDay|DwellMinutes|AreaName|StreetId|InViolation|SignPlateID|SideName|SideOfStreet|SideOfStreetCode|_duration_min      |_occupied|_event_time        |_hour|_dow|_is_weekend|_duration_outlier|_duration_min_w    |_duration_robust_z |street_marker_lumped|device_id_lumped|_duration_min_imp  |_occupied_imp|target_occupied|rolling_occ_N10|arrivals_N10|\n",
            "+-----+--------+------------+---------------+---------------------+----------+-------------------+-------------------+---------------+----+---------+---------+-----+----------+---------+------------+--------+--------+-----------+-----------+--------+------------+----------------+-------------------+---------+-------------------+-----+----+-----------+-----------------+-------------------+-------------------+--------------------+----------------+-------------------+-------------+---------------+---------------+------------+\n",
            "|1399 |23913   |1581S       |FLINDERS STREET|NULL                 |Present   |2019-09-01 08:06:25|2019-09-01 08:07:01|60.0           |4   |7        |true     |9    |1         |Night    |1.0         |Banks   |670     |false      |nan        |South   |4           |S               |0.6                |1.0      |2019-09-01 08:06:25|4.0  |6.0 |1          |0                |0.6                |-0.6641826003824093|1581S               |23913           |0.6                |1            |1              |1.0            |1.0         |\n",
            "|1399 |23913   |1581S       |FLINDERS STREET|1P SUN 7:30-18:30    |Present   |2019-09-01 20:13:22|2019-09-01 20:13:48|0.0            |16  |7        |true     |9    |1         |Afternoon|0.0         |Banks   |670     |false      |3.0        |South   |4           |S               |0.43333333333333335|1.0      |2019-09-01 20:13:22|16.0 |6.0 |1          |0                |0.43333333333333335|-0.6770793499043977|1581S               |23913           |0.43333333333333335|1            |1              |1.0            |2.0         |\n",
            "|1399 |23913   |1581S       |FLINDERS STREET|1P SUN 7:30-18:30    |Present   |2019-09-01 22:27:22|2019-09-01 22:27:36|0.0            |18  |7        |true     |9    |1         |Evening  |0.0         |Banks   |670     |false      |3.0        |South   |4           |S               |0.23333333333333334|1.0      |2019-09-01 22:27:22|18.0 |6.0 |1          |0                |0.23333333333333334|-0.692555449330784 |1581S               |23913           |0.23333333333333334|1            |1              |1.0            |3.0         |\n",
            "|1399 |23913   |1581S       |FLINDERS STREET|2P MTR M-F 9:30-16:00|NotPresent|2019-09-03 13:30:00|2019-09-03 14:49:12|4740.0         |9   |2        |false    |9    |3         |Morning  |79.0        |Banks   |670     |false      |99.0       |South   |4           |S               |79.2               |0.0      |2019-09-03 13:30:00|9.0  |1.0 |0          |0                |79.2               |5.41792447418738   |1581S               |23913           |79.2               |0            |0              |0.75           |4.0         |\n",
            "|1399 |23913   |1581S       |FLINDERS STREET|2P MTR M-F 9:30-16:00|Present   |2019-09-03 14:53:24|2019-09-03 15:07:14|840.0          |10  |2        |false    |9    |3         |Morning  |14.0        |Banks   |670     |false      |99.0       |South   |4           |S               |13.833333333333334 |1.0      |2019-09-03 14:53:24|10.0 |1.0 |0          |0                |13.833333333333334 |0.3598193116634799 |1581S               |23913           |13.833333333333334 |1            |1              |0.8            |5.0         |\n",
            "+-----+--------+------------+---------------+---------------------+----------+-------------------+-------------------+---------------+----+---------+---------+-----+----------+---------+------------+--------+--------+-----------+-----------+--------+------------+----------------+-------------------+---------+-------------------+-----+----+-----------+-----------------+-------------------+-------------------+--------------------+----------------+-------------------+-------------+---------------+---------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "df_full = df_full.withColumnRenamed(\"target_occupied\", \"label\")\n",
        "\n",
        "# Drop leakage columns (they are EXACTLY equal to the label)\n",
        "leaky_cols = [\"Status\", \"_occupied\", \"_occupied_imp\"]\n",
        "\n",
        "for c in leaky_cols:\n",
        "    if c in df_full.columns:\n",
        "        df_full = df_full.drop(c)\n",
        "\n",
        "numeric_features = [\n",
        "    \"Hour\", \"DayOfWeek\", \"IsWeekend\", \"Month\", \"DayOfMonth\",\n",
        "    \"DwellMinutes\", \"InViolation\",\n",
        "    \"_duration_min\", \"_hour\", \"_dow\", \"_is_weekend\",\n",
        "    \"_duration_min_w\", \"_duration_robust_z\",\n",
        "    \"rolling_occ_N10\", \"arrivals_N10\"\n",
        "]\n",
        "\n",
        "cat_features = [\n",
        "    \"PartOfDay\", \"AreaName\", \"SideName\", \"SideOfStreetCode\",\n",
        "    \"street_marker_lumped\", \"device_id_lumped\"\n",
        "]\n",
        "\n",
        "# Drop rows with nulls\n",
        "df_full = df_full.dropna(subset=[\"label\"] + numeric_features + cat_features)\n",
        "\n",
        "df_full = df_full.withColumn(\"label\", F.col(\"label\").cast(\"double\"))\n"
      ],
      "metadata": {
        "id": "m804mPsLAe5r"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "indexers = [\n",
        "    StringIndexer(\n",
        "        inputCol=c,\n",
        "        outputCol=f\"{c}_idx\",\n",
        "        handleInvalid=\"keep\"\n",
        "    )\n",
        "    for c in cat_features\n",
        "]\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=numeric_features + [f\"{c}_idx\" for c in cat_features],\n",
        "    outputCol=\"features_raw\"\n",
        ")\n",
        "\n",
        "scaler = StandardScaler(\n",
        "    inputCol=\"features_raw\",\n",
        "    outputCol=\"features\",\n",
        "    withMean=False,\n",
        "    withStd=True\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=indexers + [assembler, scaler])\n",
        "preprocess_model = pipeline.fit(df_full)\n",
        "df_prepared = preprocess_model.transform(df_full)\n",
        "\n",
        "train_df, test_df = df_prepared.randomSplit([0.7, 0.3], seed=42)\n"
      ],
      "metadata": {
        "id": "mL5Z0EQpAlCE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FROM NEW DATASET\n",
        "\n",
        "input_path = \"/content/smart_parking_clean.csv\"\n",
        "\n",
        "df_full = (\n",
        "    spark.read\n",
        "        .option(\"header\", True)\n",
        "        .option(\"inferSchema\", True)\n",
        "        .csv(input_path)\n",
        ")\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "df_full = df_full.withColumnRenamed(\"target_occupied\", \"label\")\n",
        "\n",
        "# Drop leakage columns\n",
        "leaky_cols = [\"Status\", \"_occupied\", \"_occupied_imp\"]\n",
        "for c in leaky_cols:\n",
        "    if c in df_full.columns:\n",
        "        df_full = df_full.drop(c)\n",
        "\n",
        "numeric_features = [\n",
        "    \"Hour\", \"DayOfWeek\", \"IsWeekend\", \"Month\", \"DayOfMonth\",\n",
        "    \"DwellMinutes\", \"InViolation\",\n",
        "    \"_duration_min\", \"_hour\", \"_dow\", \"_is_weekend\",\n",
        "    \"_duration_min_w\", \"_duration_robust_z\",\n",
        "    \"rolling_occ_N10\", \"arrivals_N10\"\n",
        "]\n",
        "\n",
        "cat_features = [\n",
        "    \"PartOfDay\", \"AreaName\", \"SideName\", \"SideOfStreetCode\",\n",
        "    \"street_marker_lumped\", \"device_id_lumped\"\n",
        "]\n",
        "\n",
        "df_full = df_full.dropna(subset=[\"label\"] + numeric_features + cat_features)\n",
        "df_full = df_full.withColumn(\"label\", F.col(\"label\").cast(\"double\"))\n"
      ],
      "metadata": {
        "id": "2lb1awOEAoG8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "indexers = [\n",
        "    StringIndexer(\n",
        "        inputCol=c,\n",
        "        outputCol=f\"{c}_idx\",\n",
        "        handleInvalid=\"keep\"\n",
        "    )\n",
        "    for c in cat_features\n",
        "]\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=numeric_features + [f\"{c}_idx\" for c in cat_features],\n",
        "    outputCol=\"features_raw\"\n",
        ")\n",
        "\n",
        "scaler = StandardScaler(\n",
        "    inputCol=\"features_raw\",\n",
        "    outputCol=\"features\",\n",
        "    withMean=False,\n",
        "    withStd=True\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=indexers + [assembler, scaler])\n",
        "preprocess_model = pipeline.fit(df_full)\n",
        "df_prepared = preprocess_model.transform(df_full)\n",
        "\n",
        "train_df, test_df = df_prepared.randomSplit([0.7, 0.3], seed=42)\n"
      ],
      "metadata": {
        "id": "GHv3rG5MPJLk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Cache to reduce recomputation\n",
        "train_df = train_df.cache()\n",
        "test_df = test_df.cache()\n",
        "\n",
        "# Base Random Forest (same as before)\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    seed=42,\n",
        "    featureSubsetStrategy=\"sqrt\"\n",
        ")\n",
        "\n",
        "# Optimize for AUC\n",
        "evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    rawPredictionCol=\"rawPrediction\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "# Small, safe param grid (4 combinations)\n",
        "paramGrid = (\n",
        "    ParamGridBuilder()\n",
        "      .addGrid(rf.numTrees, [100, 200])\n",
        "      .addGrid(rf.maxDepth, [8, 12])\n",
        "      .build()\n",
        ")\n",
        "\n",
        "# TrainValidationSplit (lighter than CrossValidator)\n",
        "tvs = TrainValidationSplit(\n",
        "    estimator=rf,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    trainRatio=0.8,\n",
        "    parallelism=1    # important for Colab memory\n",
        ")\n",
        "\n",
        "# Fit on the NEW richer train_df\n",
        "tvs_model = tvs.fit(train_df)\n",
        "best_rf_model = tvs_model.bestModel\n",
        "\n",
        "print(\"Best Random Forest params:\")\n",
        "print(\"  numTrees:\", best_rf_model.getOrDefault(\"numTrees\"))\n",
        "print(\"  maxDepth:\", best_rf_model.getOrDefault(\"maxDepth\"))\n",
        "\n",
        "# Evaluate on test set\n",
        "rf_preds = best_rf_model.transform(test_df)\n",
        "\n",
        "auc_eval = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    rawPredictionCol=\"rawPrediction\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "acc_eval = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "f1_eval = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"f1\"\n",
        ")\n",
        "\n",
        "auc = auc_eval.evaluate(rf_preds)\n",
        "acc = acc_eval.evaluate(rf_preds)\n",
        "f1 = f1_eval.evaluate(rf_preds)\n",
        "\n",
        "print(\"\\n==== Tuned Random Forest (Rich Feature Set) ====\")\n",
        "print(f\"AUC:      {auc:.4f}\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"F1:       {f1:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion matrix (prediction vs label):\")\n",
        "rf_preds.groupBy(\"prediction\", \"label\").count().orderBy(\"prediction\", \"label\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEaV5DnkPQm9",
        "outputId": "532df43a-abe4-4efa-905b-cbe296957a46"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Random Forest params:\n",
            "  numTrees: 100\n",
            "  maxDepth: 12\n",
            "\n",
            "==== Tuned Random Forest (Rich Feature Set) ====\n",
            "AUC:      0.7968\n",
            "Accuracy: 0.7157\n",
            "F1:       0.7158\n",
            "\n",
            "Confusion matrix (prediction vs label):\n",
            "+----------+-----+-----+\n",
            "|prediction|label|count|\n",
            "+----------+-----+-----+\n",
            "|       0.0|  0.0| 1555|\n",
            "|       0.0|  1.0|  646|\n",
            "|       1.0|  0.0|  603|\n",
            "|       1.0|  1.0| 1590|\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "\n",
        "gbt = GBTClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    maxIter=80,       # strong but safe number\n",
        "    maxDepth=8,       # usually best for GBT\n",
        "    stepSize=0.1,     # learning rate\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "gbt_model = gbt.fit(train_df)\n",
        "gbt_preds = gbt_model.transform(test_df)\n",
        "\n",
        "auc_eval = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    rawPredictionCol=\"rawPrediction\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "acc_eval = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "f1_eval = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"f1\"\n",
        ")\n",
        "\n",
        "auc = auc_eval.evaluate(gbt_preds)\n",
        "acc = acc_eval.evaluate(gbt_preds)\n",
        "f1 = f1_eval.evaluate(gbt_preds)\n",
        "\n",
        "print(\"\\n==== Gradient Boosted Trees ====\")\n",
        "print(f\"AUC:      {auc:.4f}\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"F1:       {f1:.4f}\")\n",
        "\n",
        "gbt_preds.groupBy(\"prediction\", \"label\").count().orderBy(\"prediction\", \"label\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-_U6y7ePbOJ",
        "outputId": "5b79e853-9d98-433b-bd20-6a317adb92fb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Gradient Boosted Trees ====\n",
            "AUC:      0.7827\n",
            "Accuracy: 0.7003\n",
            "F1:       0.7003\n",
            "+----------+-----+-----+\n",
            "|prediction|label|count|\n",
            "+----------+-----+-----+\n",
            "|       0.0|  0.0| 1526|\n",
            "|       0.0|  1.0|  685|\n",
            "|       1.0|  0.0|  632|\n",
            "|       1.0|  1.0| 1551|\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}